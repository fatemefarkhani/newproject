# -*- coding: utf-8 -*-
"""matching.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iH_gB3hJpdmLI470mdOOxx002hLO4Q7M
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install transformers

import pandas as pd
import numpy as np
import torch
import json
import gzip
import os
import random
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from transformers import BertTokenizer, BertForSequenceClassification
from sklearn.model_selection import train_test_split
from tabulate import tabulate
from tqdm import trange
from torch import nn
from torch.nn.utils.rnn import pad_sequence
from  torch.utils.data import DataLoader
from urllib.request import urlopen

"""preprocess . . ."""

batch_size = 16

from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModel.from_pretrained("bert-base-uncased")

data = pd.read_json('/content/drive/My Drive/json/dataset.json')

atext = data.answerText.values
qtext = data.questionText.values
Label = data.label.values

seq_q=qtext.tolist() 
seq_a=atext.tolist() 
labels=Label.tolist()

encoded_dict = tokenizer(seq_q, seq_a,padding=True, truncation=True,return_tensors="pt")

X_train, X_test, y_train, y_test = train_test_split(encoded_dict, labels, test_size=0.125, random_state=1)

X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.142, random_state=1)

args = TrainingArguments(
    evaluation_strategy = "epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=6,
    weight_decay=0.01,    
)

"""CMLM """

import tensorflow_hub as hub
import tensorflow as tf

english_sentences = tf.constant(["dog", "Puppies are nice.", "I enjoy taking long walks along the beach with my dog."])

preprocessor = hub.KerasLayer("https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3")
encoder = hub.KerasLayer("https://tfhub.dev/google/universal-sentence-encoder-cmlm/en-base/1")

english_embeds = encoder(preprocessor(english_sentences))["default"]

print (english_embeds)